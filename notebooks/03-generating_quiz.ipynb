{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Shaukat ali khan\\\\programming\\\\AI-Projects\\\\langchain-projects\\\\quiz-generator'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUIZ_GENERATOR_PROMPT = \"\"\"\n",
    "You will receive the contents of a particular chapter of a student's book. Your job is to create {no_questions} multiple choice question answers with {difficulty_level} difficulty using those contents and provide them in the form of python list where:\n",
    "1- All are questions are inside that list.\n",
    "2- Each question is a dictionary of values.\n",
    "3- The keys of the question dictionary should be:\n",
    "question (representing question as str)\n",
    "choices (list of choices representing each choice as str)\n",
    "correct_choice (the index of correct choice i.e. the index of correct choice form the choices list.)\n",
    "\n",
    "Note: Strictly follow the rules and don't provide anything else than the asked. Only provide a clean list ready to process further.\n",
    "\n",
    "contents: {book_content}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from notebooks.modular_code import Tokenizer\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_codebase.codebase import *\n",
    "\n",
    "\n",
    "\n",
    "def load_ollama_model(model: str = 'gemma2:2b'):\n",
    "    return OllamaLLM(model=model)\n",
    "\n",
    "def get_response_from_model(llm, prompt):\n",
    "    return llm.invoke(prompt)\n",
    "\n",
    "def get_quiz_generator_prompt():\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = ['no_questions', 'difficulty_level', 'book_content'],\n",
    "        template = QUIZ_GENERATOR_PROMPT\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def load_dataframe(df_filepath: str):\n",
    "    if is_valid_csv_path(df_filepath):\n",
    "        df = pd.read_csv(df_filepath)\n",
    "        return df\n",
    "    \n",
    "def is_valid_csv_path(path: str):\n",
    "    if is_valid_path(path):\n",
    "        if path[-3:] == 'csv':\n",
    "            return True\n",
    "        else:\n",
    "            raise Exception(f\"Not a valid csv file: {path}\")\n",
    "        \n",
    "def get_provided_unit_contents(df: pd.DataFrame, class_no: int, subject: str, unit: int):\n",
    "    \"\"\"Requires class and unit. Loads df ( will use cached using streamlit in future ) and returns the unit pages as list.\"\"\"\n",
    "    required_df = df[(df['class'] == class_no) & (df['subject'] == subject) & (df['unit_no'] == unit)]\n",
    "    required_content = list(required_df['content'])\n",
    "    return required_content\n",
    "\n",
    "def evaluate_generated_questions(response: str):\n",
    "    \"\"\"Removes keyword \"python\" and the backticks from response and then evaluates to a python object e.g. list, dict etc. and returns that.\"\"\"\n",
    "    response = response.replace(\"python\", '')\n",
    "    response = response.replace(\"```\", '')\n",
    "    response = ast.literal_eval(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_quiz_generator_prompt()\n",
    "llm = load_ollama_model()\n",
    "df = load_dataframe(r\"books_df_csvs\\Physics 9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_provided_unit_contents(df, 9, 3)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_str = \" \".join(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13407"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_token_count = tokenizer.get_content_token_count(content_str)\n",
    "content_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
